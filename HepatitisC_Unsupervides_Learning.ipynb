{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b5839fd",
      "metadata": {
        "id": "7b5839fd"
      },
      "source": [
        "# **Machine Learning II: Bayesian & Unsupervised Methods Project**\n",
        "**Ana-Maria Borduselu**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf09dc0",
      "metadata": {
        "id": "2bf09dc0"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "89f0c5eb",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "89f0c5eb"
      },
      "outputs": [],
      "source": [
        "# Core Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Dimensionality Reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# UMAP (optional â€“ requires pip install umap-learn)\n",
        "import umap\n",
        "\n",
        "# Clustering\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# Anomaly Detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "\n",
        "# Utility / Settings\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Wrangling"
      ],
      "metadata": {
        "id": "E063ylbDQL7U"
      },
      "id": "E063ylbDQL7U"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Hepatitis C dataset\n",
        "df = pd.read_csv(\"HepatitisCdata.csv\", index_col=0)\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.index = range(1, len(df) + 1)\n",
        "\n",
        "# view the first 5 rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0QWrUTdQWfz",
        "outputId": "8b9cc678-07e8-4ad1-c5b3-53e3579a3a5f"
      },
      "id": "C0QWrUTdQWfz",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Category  Age Sex   ALB   ALP   ALT   AST   BIL    CHE  CHOL   CREA  \\\n",
            "1  0=Blood Donor   32   m  38.5  52.5   7.7  22.1   7.5   6.93  3.23  106.0   \n",
            "2  0=Blood Donor   32   m  38.5  70.3  18.0  24.7   3.9  11.17  4.80   74.0   \n",
            "3  0=Blood Donor   32   m  46.9  74.7  36.2  52.6   6.1   8.84  5.20   86.0   \n",
            "4  0=Blood Donor   32   m  43.2  52.0  30.6  22.6  18.9   7.33  4.74   80.0   \n",
            "5  0=Blood Donor   32   m  39.2  74.1  32.6  24.8   9.6   9.15  4.32   76.0   \n",
            "\n",
            "    GGT  PROT  \n",
            "1  12.1  69.0  \n",
            "2  15.6  76.5  \n",
            "3  33.2  79.3  \n",
            "4  33.8  75.7  \n",
            "5  29.9  68.7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Data cleaning\n",
        "\n",
        "* Verify variable types (continuous vs categorical)\n",
        "* Handle missing values:\n",
        "\n",
        "  * Remove records with excessive missingness\n",
        "  * Impute remaining missing values using median (robust to skewness)\n",
        "* Remove obvious data entry errors (negative or impossible values)\n"
      ],
      "metadata": {
        "id": "gnGer4-1U1sL"
      },
      "id": "gnGer4-1U1sL"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2.1 DATA CLEANING\n",
        "# -----------------------------\n",
        "\n",
        "# (A) Verify variable types (categorical vs numerical)\n",
        "# Here we assume:\n",
        "# - 'Sex' and 'Category' are categorical\n",
        "# - Everything else (Age + lab markers) should be numeric\n",
        "# If your dataset uses different names (e.g. \"Sex\" as \"Sex\", \"Category\" as \"Category\"),\n",
        "# adjust the column names below accordingly.\n",
        "categorical_cols = [\"Sex\", \"Category\"]\n",
        "numeric_cols = [c for c in df.columns if c not in categorical_cols]\n",
        "\n",
        "# Ensure numeric columns are numeric (convert if they were read as strings)\n",
        "# Any non-convertible values become NaN (missing), which we will handle later.\n",
        "for c in numeric_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# (B) Handle missing values\n",
        "# Step 1: Remove records with excessive missingness\n",
        "# - \"excessive\" is a design choice. Common thresholds: 20%â€“40%.\n",
        "# Here we use 30% missing across columns as an example.\n",
        "row_missing_fraction = df.isna().mean(axis=1)\n",
        "df = df.loc[row_missing_fraction <= 0.30].copy()\n",
        "\n",
        "# Step 2: Impute remaining missing values (median is robust to skewness)\n",
        "# - We'll impute numeric columns with median.\n",
        "# - For Sex (categorical), we'll use most frequent.\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
        "df[[\"Sex\"]] = cat_imputer.fit_transform(df[[\"Sex\"]])\n",
        "\n",
        "# (C) Remove obvious data entry errors (negative or impossible values)\n",
        "# Examples:\n",
        "# - Age should be > 0\n",
        "# - lab values should generally be >= 0 (some canâ€™t be negative physiologically)\n",
        "# We'll enforce non-negativity for lab markers and a reasonable age range.\n",
        "# Adjust these rules if your dataset includes valid edge cases.\n",
        "df = df[(df[\"Age\"] > 0) & (df[\"Age\"] < 120)].copy()\n",
        "\n",
        "lab_markers = [\"ALB\",\"ALP\",\"ALT\",\"AST\",\"BIL\",\"CHE\",\"CHOL\",\"CREA\",\"GGT\",\"PROT\"]\n",
        "# Keep only lab markers that actually exist in your file (prevents key errors)\n",
        "lab_markers = [c for c in lab_markers if c in df.columns]\n",
        "\n",
        "# Remove rows where any lab marker is negative (impossible biologically)\n",
        "for c in lab_markers:\n",
        "    df = df[df[c] >= 0].copy()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNsnU9zyTxou",
        "outputId": "c7a8b40f-9b79-4744-c3e6-b7d7da0fb891"
      },
      "id": "BNsnU9zyTxou",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Category  Age Sex   ALB   ALP   ALT   AST   BIL    CHE  CHOL   CREA  \\\n",
            "1  0=Blood Donor   32   m  38.5  52.5   7.7  22.1   7.5   6.93  3.23  106.0   \n",
            "2  0=Blood Donor   32   m  38.5  70.3  18.0  24.7   3.9  11.17  4.80   74.0   \n",
            "3  0=Blood Donor   32   m  46.9  74.7  36.2  52.6   6.1   8.84  5.20   86.0   \n",
            "4  0=Blood Donor   32   m  43.2  52.0  30.6  22.6  18.9   7.33  4.74   80.0   \n",
            "5  0=Blood Donor   32   m  39.2  74.1  32.6  24.8   9.6   9.15  4.32   76.0   \n",
            "\n",
            "    GGT  PROT  \n",
            "1  12.1  69.0  \n",
            "2  15.6  76.5  \n",
            "3  33.2  79.3  \n",
            "4  33.8  75.7  \n",
            "5  29.9  68.7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Distributional analysis\n",
        "\n",
        "* Inspect univariate distributions\n",
        "* Identify skewed variables (ALT, AST, GGT, BIL)\n",
        "* Assess outliers (initially, not remove them)"
      ],
      "metadata": {
        "id": "f_PZr4A1U5em"
      },
      "id": "f_PZr4A1U5em"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2.2 DISTRIBUTIONAL ANALYSIS\n",
        "# -----------------------------\n",
        "# At this stage, we DO NOT remove outliers yet.\n",
        "# We only identify skewness and potential heavy tails to justify transformations later.\n",
        "\n",
        "# Identify skewness of each numeric variable\n",
        "skewness = df[numeric_cols].skew(numeric_only=True).sort_values(ascending=False)\n",
        "print(\"\\nSkewness (descending):\\n\", skewness)\n",
        "\n",
        "# Define known typically skewed variables in this dataset\n",
        "skewed_vars = [c for c in [\"ALT\", \"AST\", \"GGT\", \"BIL\"] if c in df.columns]\n",
        "print(\"\\nSkewed variables to consider for log transform:\", skewed_vars)\n",
        "\n",
        "# Optional: basic quantiles to see outliers (still not removing them)\n",
        "print(\"\\nQuantiles for skewed vars:\")\n",
        "print(df[skewed_vars].quantile([0.5, 0.75, 0.9, 0.95, 0.99]))\n"
      ],
      "metadata": {
        "id": "zXMz0qKwU8fb"
      },
      "id": "zXMz0qKwU8fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2.3 Scaling and transformation\n",
        "\n",
        "* Apply **log transformation** to heavily skewed variables\n",
        "* Standardize all continuous variables (z-score scaling)\n",
        "* Encode sex as binary for inclusion in numerical analysis\n",
        "\n",
        "ðŸ“Œ *Rationale:* Distance-based methods require comparable feature scales."
      ],
      "metadata": {
        "id": "gdbvgNL-U947"
      },
      "id": "gdbvgNL-U947"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2.3 SCALING AND TRANSFORMATION\n",
        "# -----------------------------\n",
        "# Rationale: distance-based algorithms (PCA, K-means, etc.) are sensitive to scale.\n",
        "# We will:\n",
        "# 1) encode Sex into numeric\n",
        "# 2) log-transform heavily skewed variables\n",
        "# 3) standardize (z-score) all continuous variables\n",
        "\n",
        "# (A) Encode Sex as binary\n",
        "# The dataset often uses 'm'/'f' or 'M'/'F'.\n",
        "# We'll map common variants. Adjust if your dataset encodes sex differently.\n",
        "sex_map = {\"m\": 1, \"M\": 1, \"male\": 1, \"Male\": 1,\n",
        "            \"f\": 0, \"F\": 0, \"female\": 0, \"Female\": 0}\n",
        "df[\"Sex_bin\"] = df[\"Sex\"].map(sex_map)\n",
        "\n",
        "# If some values didn't map (NaN), fill with the mode as a safe default.\n",
        "df[\"Sex_bin\"] = df[\"Sex_bin\"].fillna(df[\"Sex_bin\"].mode()[0]).astype(int)\n",
        "\n",
        "# (B) Separate Category for post hoc interpretation ONLY\n",
        "# This is the label-like column we do NOT use to train unsupervised models.\n",
        "y_category = df[\"Category\"].copy()\n",
        "\n",
        "# (C) Create the feature matrix X (exclude Category and original Sex)\n",
        "# Keep Sex_bin instead.\n",
        "feature_cols = [\"Age\", \"Sex_bin\"] + lab_markers\n",
        "X = df[feature_cols].copy()\n",
        "\n",
        "# (D) Log-transform heavily skewed variables\n",
        "# Use log1p(x) = log(1 + x), safe when x can be 0.\n",
        "for c in skewed_vars:\n",
        "    if c in X.columns:\n",
        "        X[c] = np.log1p(X[c])\n",
        "\n",
        "# (E) Standardize all features (mean=0, std=1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Optional: put back into a DataFrame for readability\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=df.index)\n",
        "\n",
        "print(\"\\nFinal feature matrix ready for unsupervised learning:\")\n",
        "print(\"X_scaled shape:\", X_scaled.shape)\n",
        "print(\"Category kept separately (post hoc only). Unique categories:\", sorted(y_category.unique()))\n",
        "\n",
        "# ============================================================\n",
        "# At this point:\n",
        "# - X_scaled is ready for PCA, clustering, anomaly detection\n",
        "# - y_category is saved only to interpret clusters afterwards\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "id": "Eitp-loXVDpN"
      },
      "id": "Eitp-loXVDpN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "2nMhPp0iVGd6"
      },
      "id": "2nMhPp0iVGd6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives\n",
        "\n",
        "* Understand baseline variability\n",
        "* Identify correlations and redundancy\n",
        "* Detect early structure or anomalies\n",
        "\n",
        "### Methods\n",
        "\n",
        "* Summary statistics per variable\n",
        "* Correlation matrix and heatmap\n",
        "* Pairwise plots (selected variables)\n",
        "\n",
        "### Outputs\n",
        "\n",
        "* Identification of strongly correlated markers (ALTâ€“AST, ALBâ€“PROT)\n",
        "* Justification for dimensionality reduction"
      ],
      "metadata": {
        "id": "LMZ8f2AHVHuT"
      },
      "id": "LMZ8f2AHVHuT"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# ASSUMPTION / INPUTS FROM PREVIOUS STEP\n",
        "# ------------------------------------------------------------\n",
        "# You should already have:\n",
        "# - df: cleaned dataframe (still contains original variables)\n",
        "# - X: feature dataframe before scaling/transforms (Age, Sex_bin, lab markers, log transforms applied if you chose)\n",
        "# - X_scaled: standardized features (DataFrame)\n",
        "# - y_category: stored separately for post hoc interpretation\n",
        "#\n",
        "# If you ran Part A, you have:\n",
        "#   df, X, X_scaled, y_category\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# ============================================================\n",
        "# Objectives:\n",
        "# 1) Understand baseline variability (ranges, distributions)\n",
        "# 2) Identify correlations and redundancy (highly correlated markers)\n",
        "# 3) Detect early structure or anomalies (extreme values, clusters visually)\n"
      ],
      "metadata": {
        "id": "QITXlV2lVNEK"
      },
      "id": "QITXlV2lVNEK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3.1 Summary statistics\n",
        "# -----------------------------\n",
        "# Summary stats help you understand:\n",
        "# - central tendency (mean/median)\n",
        "# - spread (std/IQR)\n",
        "# - extreme values (min/max)\n",
        "# This is crucial for medical lab data because many markers are skewed.\n",
        "\n",
        "print(\"\\n--- Summary statistics (features only) ---\")\n",
        "print(X.describe().T)\n",
        "\n",
        "# Optional: more robust statistics (median, IQR) for skewed variables\n",
        "print(\"\\n--- Robust stats (median + IQR) ---\")\n",
        "median = X.median(numeric_only=True)\n",
        "q1 = X.quantile(0.25, numeric_only=True)\n",
        "q3 = X.quantile(0.75, numeric_only=True)\n",
        "iqr = q3 - q1\n",
        "robust_stats = pd.DataFrame({\"median\": median, \"Q1\": q1, \"Q3\": q3, \"IQR\": iqr})\n",
        "print(robust_stats)"
      ],
      "metadata": {
        "id": "N-JN-W3GVOk3"
      },
      "id": "N-JN-W3GVOk3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3.2 Correlation matrix (redundancy detection)\n",
        "# -----------------------------\n",
        "# Correlations reveal redundancy:\n",
        "# - Example: ALT and AST often move together (both reflect liver injury)\n",
        "# - ALB and PROT can correlate (albumin is a major component of total protein)\n",
        "#\n",
        "# We compute correlations on X (interpretable). Correlation is scale-invariant.\n",
        "# If you want, you can also compute on X_scaled.\n",
        "\n",
        "corr = X.corr(numeric_only=True)\n",
        "\n",
        "print(\"\\n--- Top absolute correlations (excluding self-correlations) ---\")\n",
        "# Extract upper triangle correlations only (avoid duplicates)\n",
        "corr_abs = corr.abs()\n",
        "upper = corr_abs.where(np.triu(np.ones(corr_abs.shape), k=1).astype(bool))\n",
        "top_pairs = upper.stack().sort_values(ascending=False).head(15)\n",
        "print(top_pairs)\n"
      ],
      "metadata": {
        "id": "g0sGG7V4VR2d"
      },
      "id": "g0sGG7V4VR2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3.3 Heatmap of correlations\n",
        "# -----------------------------\n",
        "# A heatmap is a fast way to see blocks of correlated features.\n",
        "# If seaborn isn't available, you can skip this section.\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, annot=False, cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Correlation Matrix (Features)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cOussnl_VUjE"
      },
      "id": "cOussnl_VUjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3.4 Pairwise plots (selected variables)\n",
        "# -----------------------------\n",
        "# Pairwise plots help detect:\n",
        "# - obvious groupings (early cluster structure)\n",
        "# - outliers (extreme values)\n",
        "# - non-linear relationships\n",
        "#\n",
        "# Since full pairplots can be heavy, select a small set of clinically meaningful markers.\n",
        "# Recommended set: ALT, AST, GGT, BIL, ALB, PROT (+ Age)\n",
        "selected = [c for c in [\"Age\", \"ALB\", \"PROT\", \"ALT\", \"AST\", \"BIL\", \"GGT\"] if c in X.columns]\n",
        "\n",
        "print(\"\\nPairplot variables used:\", selected)\n",
        "\n",
        "if len(selected) >= 2:\n",
        "    # Pairplot can take time on large datasets, but this dataset is usually manageable.\n",
        "    # We avoid using Category here to respect the unsupervised philosophy.\n",
        "    # (You may color by Category later ONLY for post hoc interpretation.)\n",
        "    sns.pairplot(X[selected], corner=True, plot_kws={\"s\": 10, \"alpha\": 0.6})\n",
        "    plt.suptitle(\"Pairwise Plots (Selected Biomarkers)\", y=1.02)\n",
        "    plt.show()\n",
        "else:\n",
        "    # Minimal alternative: scatterplots for a few key relationships\n",
        "    # ALT vs AST (expected strong correlation)\n",
        "    if \"ALT\" in X.columns and \"AST\" in X.columns:\n",
        "        plt.figure()\n",
        "        plt.scatter(X[\"ALT\"], X[\"AST\"], s=10, alpha=0.6)\n",
        "        plt.xlabel(\"ALT\")\n",
        "        plt.ylabel(\"AST\")\n",
        "        plt.title(\"ALT vs AST (Expected strong correlation)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # ALB vs PROT (potential redundancy)\n",
        "    if \"ALB\" in X.columns and \"PROT\" in X.columns:\n",
        "        plt.figure()\n",
        "        plt.scatter(X[\"ALB\"], X[\"PROT\"], s=10, alpha=0.6)\n",
        "        plt.xlabel(\"ALB\")\n",
        "        plt.ylabel(\"PROT\")\n",
        "        plt.title(\"ALB vs PROT (Potential redundancy)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "LaiLNAlgVYV-"
      },
      "id": "LaiLNAlgVYV-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3.5 Early anomaly detection (EDA-level)\n",
        "# -----------------------------\n",
        "# At EDA stage, we do NOT run full anomaly algorithms yet.\n",
        "# We just flag extreme values using quantiles to see whether the dataset\n",
        "# contains heavy tails / extreme cases that may influence clustering.\n",
        "\n",
        "high_quantile = 0.99  # you can also check 0.995 or 0.999\n",
        "extreme_flags = {}\n",
        "\n",
        "for c in selected:\n",
        "    if c in X.columns and pd.api.types.is_numeric_dtype(X[c]):\n",
        "        threshold = X[c].quantile(high_quantile)\n",
        "        extreme_flags[c] = int((X[c] > threshold).sum())\n",
        "\n",
        "print(f\"\\n--- Number of points above the {high_quantile:.2f} quantile (selected vars) ---\")\n",
        "for k, v in extreme_flags.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "8-dGBPHdVa9r"
      },
      "id": "8-dGBPHdVa9r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# 3.6 Expected EDA Outputs (to write in your report)\n",
        "# ============================================================\n",
        "# From the top correlations, you will usually observe:\n",
        "# - ALTâ€“AST strong positive correlation (both liver injury enzymes)\n",
        "# - ALBâ€“PROT positive correlation (albumin contributes to total protein)\n",
        "#\n",
        "# These findings justify dimensionality reduction because:\n",
        "# - correlated variables increase redundancy in feature space\n",
        "# - PCA can compress correlated markers into fewer latent components\n",
        "#\n",
        "# You can copy the printed \"top_pairs\" into your report narrative.\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "id": "6SP3lNQRVcqE"
      },
      "id": "6SP3lNQRVcqE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dimensionality Reduction and Latent Structure Discovery**"
      ],
      "metadata": {
        "id": "2qIug1CXVeuj"
      },
      "id": "2qIug1CXVeuj"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Hepatitis C Unsupervised Learning Project\n",
        "# Part C â€” Dimensionality Reduction & Latent Structure Discovery\n",
        "#   4.1 PCA (for modeling + interpretability)\n",
        "#   4.2 UMAP / t-SNE (visualization only)\n",
        "#   (Corrected: Category handling for matplotlib coloring)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ASSUMPTION / INPUTS FROM PREVIOUS STEPS\n",
        "# ------------------------------------------------------------\n",
        "# You should already have:\n",
        "# - X_scaled: standardized feature matrix (DataFrame)\n",
        "# - y_category: stored separately for post hoc interpretation (Series)\n",
        "# ------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "jrVl4i7UVf_f"
      },
      "id": "jrVl4i7UVf_f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0) FIX CATEGORY FOR PLOTTING\n",
        "# ============================================================\n",
        "# Your Category may be stored as strings like \"0=Blood Donor\".\n",
        "# Matplotlib scatter(c=...) requires numeric values.\n",
        "# We keep BOTH:\n",
        "# - y_category_label: original labels (for legends / tables)\n",
        "# - y_category_num: numeric encoding (for colormap coloring)\n",
        "\n",
        "y_category_label = y_category.astype(str)\n",
        "\n",
        "# Extract leading digit(s) if Category looks like \"0=Blood Donor\"\n",
        "# If Category is already numeric, this still works.\n",
        "y_category_num = (\n",
        "    y_category_label\n",
        "    .str.extract(r\"^(\\d+)\")[0]\n",
        "    .fillna(y_category_label)         # fallback if it doesn't start with digits\n",
        ")\n",
        "\n",
        "# Convert to numeric safely\n",
        "y_category_num = pd.to_numeric(y_category_num, errors=\"coerce\")\n",
        "\n",
        "# If any values still failed conversion, map unique labels to integers\n",
        "if y_category_num.isna().any():\n",
        "    label_map = {lab: i for i, lab in enumerate(sorted(y_category_label.unique()))}\n",
        "    y_category_num = y_category_label.map(label_map).astype(int)\n",
        "else:\n",
        "    y_category_num = y_category_num.astype(int)\n",
        "\n",
        "print(\"Category labels example:\", y_category_label.unique()[:5])\n",
        "print(\"Category numeric codes example:\", sorted(y_category_num.unique()))\n",
        "\n"
      ],
      "metadata": {
        "id": "-wLAnvKNVhfH"
      },
      "id": "-wLAnvKNVhfH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Principal Component Analysis (PCA)\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Reduce dimensionality\n",
        "* Identify dominant axes of variation\n",
        "* Improve clustering stability\n",
        "\n",
        "**Procedure**\n",
        "\n",
        "* Apply PCA on standardized data\n",
        "* Examine explained variance ratio\n",
        "* Retain components explaining ~70â€“80% variance\n",
        "\n",
        "**Interpretation**\n",
        "\n",
        "* Analyze PCA loadings to link components to physiological processes (e.g. inflammation vs synthetic function)"
      ],
      "metadata": {
        "id": "q_hFr105Vjsy"
      },
      "id": "q_hFr105Vjsy"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4.1 PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# 4.1.1 Fit PCA on standardized data\n",
        "# -----------------------------\n",
        "pca_full = PCA(n_components=None, random_state=42)\n",
        "X_pca_full = pca_full.fit_transform(X_scaled)\n",
        "\n",
        "explained = pca_full.explained_variance_ratio_\n",
        "cum_explained = np.cumsum(explained)\n",
        "\n",
        "print(\"\\n--- PCA explained variance ratio (first 10 PCs) ---\")\n",
        "for i, v in enumerate(explained[:10], start=1):\n",
        "    print(f\"PC{i}: {v:.4f} (cumulative: {cum_explained[i-1]:.4f})\")\n"
      ],
      "metadata": {
        "id": "9-9-uetmVkop"
      },
      "id": "9-9-uetmVkop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4.1.2 Plot cumulative explained variance\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(range(1, len(cum_explained) + 1), cum_explained, marker=\"o\")\n",
        "plt.axhline(0.70, linestyle=\"--\")\n",
        "plt.axhline(0.80, linestyle=\"--\")\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"PCA â€” Cumulative Explained Variance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0h-rKJnJVnS4"
      },
      "id": "0h-rKJnJVnS4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4.1.3 Choose number of components (~70â€“80% variance)\n",
        "# -----------------------------\n",
        "target_variance = 0.80\n",
        "k = int(np.argmax(cum_explained >= target_variance) + 1)\n",
        "print(f\"\\nChosen number of components k = {k} to reach â‰¥ {target_variance*100:.0f}% variance.\")\n",
        "\n",
        "pca = PCA(n_components=k, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "pc_cols = [f\"PC{i}\" for i in range(1, k + 1)]\n",
        "X_pca_df = pd.DataFrame(X_pca, columns=pc_cols, index=X_scaled.index)\n",
        "\n",
        "print(\"\\nX_pca_df shape:\", X_pca_df.shape)"
      ],
      "metadata": {
        "id": "9dsTdmRVVo3c"
      },
      "id": "9dsTdmRVVo3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4.1.4 PCA loadings (interpretation)\n",
        "# -----------------------------\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    index=X_scaled.columns,\n",
        "    columns=pc_cols\n",
        ")\n",
        "\n",
        "n_top = 5\n",
        "pcs_to_show = min(3, k)\n",
        "\n",
        "print(\"\\n--- PCA Loadings Interpretation (Top contributors) ---\")\n",
        "for pc in pc_cols[:pcs_to_show]:\n",
        "    top_pos = loadings[pc].sort_values(ascending=False).head(n_top)\n",
        "    top_neg = loadings[pc].sort_values(ascending=True).head(n_top)\n",
        "    print(f\"\\n{pc} â€” Top + contributors:\\n{top_pos}\")\n",
        "    print(f\"{pc} â€” Top - contributors:\\n{top_neg}\")\n",
        "\n",
        "# Optional: barplot for PC1 loadings\n",
        "pc_to_plot = \"PC1\"\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(loadings.index, loadings[pc_to_plot].values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel(\"Loading weight\")\n",
        "plt.title(f\"PCA Loadings â€” {pc_to_plot}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hdnQ-9O8VqXG"
      },
      "id": "hdnQ-9O8VqXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Non-linear embeddings (Visualization only)\n",
        "\n",
        "**Methods**\n",
        "\n",
        "* UMAP\n",
        "* t-SNE (optional comparison)\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Visualize local and global structure\n",
        "* Identify gradients, overlap, and density variations\n",
        "\n",
        "ðŸ“Œ *Important:* These embeddings are **not used for clustering**, only for interpretation."
      ],
      "metadata": {
        "id": "uZtqwS9BVsmM"
      },
      "id": "uZtqwS9BVsmM"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4.2 NON-LINEAR EMBEDDINGS (VISUALIZATION ONLY)\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# 4.2.1 t-SNE (optional comparison)\n",
        "# -----------------------------\n",
        "tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=30,\n",
        "    learning_rate=\"auto\",\n",
        "    init=\"pca\",\n",
        "    random_state=42\n",
        ")\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=10, alpha=0.6)\n",
        "plt.xlabel(\"t-SNE 1\")\n",
        "plt.ylabel(\"t-SNE 2\")\n",
        "plt.title(\"t-SNE Embedding (Visualization Only)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3EUuVGvcVt4D"
      },
      "id": "3EUuVGvcVt4D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4.2.2 UMAP (recommended visualization)\n",
        "# -----------------------------\n",
        "reducer = umap.UMAP(\n",
        "        n_components=2,\n",
        "        n_neighbors=15,\n",
        "        min_dist=0.1,\n",
        "        random_state=42)\n",
        "\n",
        "X_umap = reducer.fit_transform(X_scaled)\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X_umap[:, 0], X_umap[:, 1], s=10, alpha=0.6)\n",
        "plt.xlabel(\"UMAP 1\")\n",
        "plt.ylabel(\"UMAP 2\")\n",
        "plt.title(\"UMAP Embedding (Visualization Only)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_O8oTmhGVvln"
      },
      "id": "_O8oTmhGVvln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# OPTIONAL: POST HOC COLORING BY CATEGORY (INTERPRETATION ONLY)\n",
        "# ============================================================\n",
        "\n",
        "def scatter_by_category_numeric(X_2d, title, y_num):\n",
        "    \"\"\"2D scatter colored by numeric category codes (safe for matplotlib).\"\"\"\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_num, s=10, alpha=0.7)\n",
        "    plt.xlabel(\"Dim 1\")\n",
        "    plt.ylabel(\"Dim 2\")\n",
        "    plt.title(title + \" (colored by Category â€” post hoc)\")\n",
        "    plt.colorbar(scatter, label=\"Category code\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def scatter_by_category_labels(X_2d, title, y_lab):\n",
        "    \"\"\"2D scatter with a legend using the original string labels.\"\"\"\n",
        "    labs = pd.Series(y_lab).astype(str)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    for lab in sorted(labs.unique()):\n",
        "        mask = labs == lab\n",
        "        plt.scatter(X_2d[mask, 0], X_2d[mask, 1], s=10, alpha=0.7, label=lab)\n",
        "    plt.xlabel(\"Dim 1\")\n",
        "    plt.ylabel(\"Dim 2\")\n",
        "    plt.title(title + \" (post hoc labels)\")\n",
        "    plt.legend(markerscale=2, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Use numeric coloring (fast + safe)\n",
        "scatter_by_category_numeric(X_tsne, \"t-SNE Embedding\", y_category_num)\n",
        "\n",
        "if X_umap is not None:\n",
        "    scatter_by_category_numeric(X_umap, \"UMAP Embedding\", y_category_num)\n",
        "\n",
        "# If you prefer readable legends (slower but nicer for reports), use:\n",
        "# scatter_by_category_labels(X_tsne, \"t-SNE Embedding\", y_category_label)\n",
        "# if X_umap is not None:\n",
        "#     scatter_by_category_labels(X_umap, \"UMAP Embedding\", y_category_label)\n",
        "\n"
      ],
      "metadata": {
        "id": "-KVezaJQVxPY"
      },
      "id": "-KVezaJQVxPY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# OUTPUTS FOR NEXT STEPS\n",
        "# ============================================================\n",
        "# - X_pca_df : PCA-reduced representation (use for clustering stability)\n",
        "# - loadings : interpret PCs (inflammation vs synthetic function)\n",
        "# - X_tsne / X_umap : visualization only\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "id": "S-ombjirVzcR"
      },
      "id": "S-ombjirVzcR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Patient Stratification via Clustering**"
      ],
      "metadata": {
        "id": "KDOCIWQKV2Fa"
      },
      "id": "KDOCIWQKV2Fa"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Hepatitis C Unsupervised Learning Project\n",
        "# Part D â€” Patient Stratification via Clustering\n",
        "#   5.1 K-Means + Elbow + Silhouette\n",
        "#   5.2 Gaussian Mixture Models (GMM) + BIC + soft membership\n",
        "#   5.3 Hierarchical clustering + dendrogram\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ASSUMPTION / INPUTS FROM PREVIOUS STEPS\n",
        "# ------------------------------------------------------------\n",
        "# You should already have:\n",
        "# - X_pca_df : PCA-reduced representation (DataFrame)\n",
        "# - (optional) y_category, y_category_num for later post hoc interpretation\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# We cluster on PCA-reduced data for:\n",
        "# - noise reduction\n",
        "# - improved stability\n",
        "# - reduced dimensionality\n",
        "# ------------------------------------------------------------\n",
        "X_cluster = X_pca_df.copy()"
      ],
      "metadata": {
        "id": "xgnWInjSV3TD"
      },
      "id": "xgnWInjSV3TD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 K-Means Clustering\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Establish a baseline clustering solution\n",
        "\n",
        "**Procedure**\n",
        "\n",
        "* Apply K-means on PCA-reduced data\n",
        "* Select number of clusters using:\n",
        "\n",
        "  * Elbow method\n",
        "  * Silhouette score\n",
        "\n",
        "**Output**\n",
        "\n",
        "* Cluster centroids\n",
        "* Cluster sizes\n",
        "* Silhouette values\n"
      ],
      "metadata": {
        "id": "FqOMu54gV6cy"
      },
      "id": "FqOMu54gV6cy"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.1 K-MEANS CLUSTERING\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# 5.1.1 Elbow Method\n",
        "# -----------------------------\n",
        "# Purpose:\n",
        "# - Examine inertia (within-cluster sum of squares)\n",
        "# - Identify diminishing returns when increasing k\n",
        "\n",
        "inertias = []\n",
        "k_range = range(2, 11)  # try 2 to 10 clusters\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
        "    kmeans.fit(X_cluster)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(list(k_range), inertias, marker=\"o\")\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Inertia (Within-cluster SSE)\")\n",
        "plt.title(\"Elbow Method for K-Means\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dfR7nfgqV70W"
      },
      "id": "dfR7nfgqV70W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5.1.2 Silhouette Score\n",
        "# -----------------------------\n",
        "# Purpose:\n",
        "# - Measure cluster separation and cohesion\n",
        "# - Range: [-1, 1]\n",
        "# - Higher = better defined clusters\n",
        "\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
        "    labels = kmeans.fit_predict(X_cluster)\n",
        "    score = silhouette_score(X_cluster, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(list(k_range), silhouette_scores, marker=\"o\")\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Score vs k\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Choose optimal k based on silhouette score (and sanity-check with elbow)\n",
        "optimal_k = int(list(k_range)[int(np.argmax(silhouette_scores))])\n",
        "print(f\"Selected k = {optimal_k} based on silhouette score.\")\n"
      ],
      "metadata": {
        "id": "TrxmljuFV_2c"
      },
      "id": "TrxmljuFV_2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5.1.3 Final K-Means model\n",
        "# -----------------------------\n",
        "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)\n",
        "cluster_labels_km = kmeans_final.fit_predict(X_cluster)\n",
        "\n",
        "# Store cluster assignments\n",
        "X_cluster[\"Cluster_KMeans\"] = cluster_labels_km"
      ],
      "metadata": {
        "id": "7De2AdagWBu2"
      },
      "id": "7De2AdagWBu2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Outputs (K-Means)\n",
        "# -----------------------------\n",
        "\n",
        "# Cluster sizes\n",
        "cluster_sizes = X_cluster[\"Cluster_KMeans\"].value_counts().sort_index()\n",
        "print(\"\\nCluster sizes (K-Means):\")\n",
        "print(cluster_sizes)\n",
        "\n",
        "# Cluster centroids in PCA space\n",
        "centroids_km = pd.DataFrame(\n",
        "    kmeans_final.cluster_centers_,\n",
        "    columns=X_cluster.columns.drop(\"Cluster_KMeans\")\n",
        ")\n",
        "print(\"\\nCluster centroids (K-Means, PCA space):\")\n",
        "print(centroids_km)\n",
        "\n",
        "final_sil = silhouette_score(X_cluster.drop(columns=[\"Cluster_KMeans\"]), cluster_labels_km)\n",
        "print(\"\\nFinal silhouette score (K-Means):\", final_sil)\n",
        "\n",
        "# Optional: visualize clusters in PC1/PC2\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X_cluster[\"PC1\"], X_cluster[\"PC2\"], c=X_cluster[\"Cluster_KMeans\"], s=10, alpha=0.7)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"K-Means Clusters in PCA Space\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xQXxWy-WWDAH"
      },
      "id": "xQXxWy-WWDAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Gaussian Mixture Models (GMM)\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Model overlapping clusters probabilistically\n",
        "\n",
        "**Procedure**\n",
        "\n",
        "* Fit GMM with varying number of components\n",
        "* Select model using BIC\n",
        "* Analyze soft cluster memberships\n",
        "\n",
        "ðŸ“Œ *Rationale:* Reflects biological overlap between disease stages.\n"
      ],
      "metadata": {
        "id": "JM7h2oOqWFEh"
      },
      "id": "JM7h2oOqWFEh"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.2 GAUSSIAN MIXTURE MODELS (GMM)\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# 5.2.1 Select number of components using BIC\n",
        "# -----------------------------\n",
        "# Purpose:\n",
        "# - Choose the number of mixture components that best balances\n",
        "#   fit and complexity (lower BIC is better).\n",
        "\n",
        "bic_scores = []\n",
        "gmm_models = []\n",
        "\n",
        "X_gmm = X_cluster.drop(columns=[\"Cluster_KMeans\"])\n",
        "\n",
        "for k in k_range:\n",
        "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
        "    gmm.fit(X_gmm)\n",
        "    bic_scores.append(gmm.bic(X_gmm))\n",
        "    gmm_models.append(gmm)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(list(k_range), bic_scores, marker=\"o\")\n",
        "plt.xlabel(\"Number of components\")\n",
        "plt.ylabel(\"BIC (lower is better)\")\n",
        "plt.title(\"GMM Model Selection (BIC)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "optimal_gmm_k = int(list(k_range)[int(np.argmin(bic_scores))])\n",
        "print(f\"Selected GMM components = {optimal_gmm_k}\")\n"
      ],
      "metadata": {
        "id": "6FoSWgidWGf4"
      },
      "id": "6FoSWgidWGf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5.2.2 Fit final GMM\n",
        "# -----------------------------\n",
        "gmm_final = GaussianMixture(n_components=optimal_gmm_k, random_state=42)\n",
        "gmm_final.fit(X_gmm)\n",
        "\n",
        "cluster_labels_gmm = gmm_final.predict(X_gmm)\n",
        "cluster_probs = gmm_final.predict_proba(X_gmm)\n",
        "\n",
        "X_cluster[\"Cluster_GMM\"] = cluster_labels_gmm\n",
        "\n",
        "# -----------------------------\n",
        "# Outputs (GMM)\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\nFirst 5 soft cluster probability vectors (GMM):\")\n",
        "print(pd.DataFrame(cluster_probs[:5], columns=[f\"GMM_{i}\" for i in range(optimal_gmm_k)]))\n",
        "\n",
        "print(\"\\nCluster sizes (GMM):\")\n",
        "print(pd.Series(cluster_labels_gmm).value_counts().sort_index())\n",
        "\n",
        "# Optional: visualize GMM clusters in PC1/PC2\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X_cluster[\"PC1\"], X_cluster[\"PC2\"], c=X_cluster[\"Cluster_GMM\"], s=10, alpha=0.7)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"GMM Clusters in PCA Space\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y0ggi2OCWIIj"
      },
      "id": "Y0ggi2OCWIIj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Hierarchical Clustering\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Explore multi-scale structure\n",
        "* Visualize progression patterns\n",
        "\n",
        "**Procedure**\n",
        "\n",
        "* Apply agglomerative clustering\n",
        "* Generate dendrogram\n",
        "* Compare different cut heights"
      ],
      "metadata": {
        "id": "fWNf09yiWJ6y"
      },
      "id": "fWNf09yiWJ6y"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.3 HIERARCHICAL CLUSTERING\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# 5.3.1 Generate linkage matrix + dendrogram\n",
        "# -----------------------------\n",
        "# Purpose:\n",
        "# - Explore multi-scale structure\n",
        "# - Visualize potential progression patterns\n",
        "#\n",
        "# IMPORTANT: Use ONLY PCA features (exclude cluster label columns)\n",
        "X_hc = X_cluster.drop(columns=[\"Cluster_KMeans\", \"Cluster_GMM\"])\n",
        "\n",
        "Z = linkage(X_hc, method=\"ward\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(Z, truncate_mode=\"level\", p=5)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram (Truncated)\")\n",
        "plt.xlabel(\"Samples (truncated view)\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wlLxUPKVWLBp"
      },
      "id": "wlLxUPKVWLBp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5.3.2 Agglomerative clustering (Ward)\n",
        "# -----------------------------\n",
        "# Choose n_clusters (we use the same as K-Means for comparison)\n",
        "agg = AgglomerativeClustering(n_clusters=optimal_k, linkage=\"ward\")\n",
        "cluster_labels_hc = agg.fit_predict(X_hc)\n",
        "\n",
        "X_cluster[\"Cluster_Hierarchical\"] = cluster_labels_hc\n",
        "\n",
        "print(\"\\nCluster sizes (Hierarchical):\")\n",
        "print(pd.Series(cluster_labels_hc).value_counts().sort_index())\n",
        "\n",
        "# Optional: visualize hierarchical clusters in PC1/PC2\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X_cluster[\"PC1\"], X_cluster[\"PC2\"], c=X_cluster[\"Cluster_Hierarchical\"], s=10, alpha=0.7)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"Hierarchical Clusters in PCA Space\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mslrgcVOWMVM"
      },
      "id": "mslrgcVOWMVM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# OUTPUTS FOR NEXT STEPS\n",
        "# ============================================================\n",
        "# - X_cluster now contains:\n",
        "#   * PCA components (PC1..PCk)\n",
        "#   * Cluster_KMeans\n",
        "#   * Cluster_GMM\n",
        "#   * Cluster_Hierarchical\n",
        "#\n",
        "# Use X_cluster + y_category (post hoc) in Section 6 (continuum analysis)\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "id": "VBHyh0WUWN-b"
      },
      "id": "VBHyh0WUWN-b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Disease Progression and Latent Continuum Analysis**"
      ],
      "metadata": {
        "id": "Es0BCj0jWP6p"
      },
      "id": "Es0BCj0jWP6p"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6) DISEASE PROGRESSION & LATENT CONTINUUM ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ASSUMPTION\n",
        "# ------------------------------------------------------------\n",
        "# You already have:\n",
        "# - X_pca_df  â†’ PCA representation (DataFrame)\n",
        "# - X_cluster â†’ dataframe containing cluster labels (DataFrame)\n",
        "# - y_category â†’ original diagnostic categories (post hoc only; may be strings like \"0=Blood Donor\")\n",
        "# - loadings â†’ PCA loadings (DataFrame)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# ============================================================\n",
        "# 0) FIX CATEGORY TYPE FOR PLOTTING (IMPORTANT)\n",
        "# ============================================================\n",
        "# Matplotlib scatter(c=...) needs numeric values, but Category might be strings.\n",
        "# We'll create both numeric and label versions:\n",
        "y_category_label = pd.Series(y_category).astype(str)\n",
        "\n",
        "# Try to extract a leading integer from labels like \"0=Blood Donor\"\n",
        "y_category_num = y_category_label.str.extract(r\"^(\\d+)\")[0]\n",
        "y_category_num = pd.to_numeric(y_category_num, errors=\"coerce\")\n",
        "\n",
        "# If extraction fails (still NaNs), map unique labels to integer codes\n",
        "if y_category_num.isna().any():\n",
        "    label_map = {lab: i for i, lab in enumerate(sorted(y_category_label.unique()))}\n",
        "    y_category_num = y_category_label.map(label_map).astype(int)\n",
        "else:\n",
        "    y_category_num = y_category_num.astype(int)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# For clarity, merge everything into one analysis dataframe\n",
        "# ------------------------------------------------------------\n",
        "analysis_df = X_pca_df.copy()\n",
        "\n",
        "analysis_df[\"Cluster_KMeans\"] = X_cluster[\"Cluster_KMeans\"].values\n",
        "analysis_df[\"Cluster_GMM\"] = X_cluster[\"Cluster_GMM\"].values\n",
        "analysis_df[\"Cluster_Hierarchical\"] = X_cluster[\"Cluster_Hierarchical\"].values\n",
        "\n",
        "analysis_df[\"Category_label\"] = y_category_label.values\n",
        "analysis_df[\"Category_num\"] = y_category_num.values\n"
      ],
      "metadata": {
        "id": "j_vWnu4MWPkk"
      },
      "id": "j_vWnu4MWPkk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives\n",
        "\n",
        "* Determine whether disease structure is discrete or continuous\n",
        "* Understand cluster ordering\n",
        "\n",
        "### Methods\n",
        "\n",
        "* Project clusters onto PCA components\n",
        "* Examine cluster centroids along principal axes\n",
        "* Overlay post hoc diagnostic categories\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "* Identify monotonic trends along severity axes\n",
        "* Assess overlap between adjacent groups"
      ],
      "metadata": {
        "id": "FM7jghypWSN8"
      },
      "id": "FM7jghypWSN8"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# 6.1 PROJECT CLUSTERS ONTO PCA SPACE\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(\n",
        "    analysis_df[\"PC1\"],\n",
        "    analysis_df[\"PC2\"],\n",
        "    c=analysis_df[\"Cluster_KMeans\"],\n",
        "    s=10,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"K-Means Clusters in PCA Space\")\n",
        "plt.colorbar(label=\"Cluster (K-Means)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Compare GMM clusters in same space\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(\n",
        "    analysis_df[\"PC1\"],\n",
        "    analysis_df[\"PC2\"],\n",
        "    c=analysis_df[\"Cluster_GMM\"],\n",
        "    s=10,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"GMM Clusters in PCA Space\")\n",
        "plt.colorbar(label=\"Cluster (GMM)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5gyDRWYTWTXL"
      },
      "id": "5gyDRWYTWTXL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.2 EXAMINE CLUSTER CENTROIDS ALONG PRINCIPAL AXES\n",
        "# ============================================================\n",
        "\n",
        "# Compute centroids in PCA space (you can extend to PC3, PC4 if needed)\n",
        "centroids_pca = analysis_df.groupby(\"Cluster_KMeans\")[[\"PC1\", \"PC2\"]].mean()\n",
        "\n",
        "print(\"\\nCluster centroids (K-Means, PCA space):\")\n",
        "print(centroids_pca)\n",
        "\n",
        "# Plot centroids for PC1 (often a candidate severity axis)\n",
        "plt.figure()\n",
        "plt.bar(centroids_pca.index.astype(str), centroids_pca[\"PC1\"])\n",
        "plt.xlabel(\"Cluster (K-Means)\")\n",
        "plt.ylabel(\"Mean PC1\")\n",
        "plt.title(\"Cluster Ordering Along PC1 (K-Means)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# OPTIONAL: Show ordering explicitly\n",
        "centroid_order = centroids_pca[\"PC1\"].sort_values().index.tolist()\n",
        "print(\"\\nClusters ordered by increasing mean PC1 (K-Means):\", centroid_order)"
      ],
      "metadata": {
        "id": "KKtv3aTzWVYh"
      },
      "id": "KKtv3aTzWVYh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.3 LINK PCA AXES TO PHYSIOLOGY\n",
        "# ============================================================\n",
        "\n",
        "# Loadings interpretation helper: show strongest contributors for a PC\n",
        "def show_top_loadings(loadings_df, pc=\"PC1\", n=7):\n",
        "    print(f\"\\nTop + loadings for {pc}:\")\n",
        "    print(loadings_df[pc].sort_values(ascending=False).head(n))\n",
        "    print(f\"\\nTop - loadings for {pc}:\")\n",
        "    print(loadings_df[pc].sort_values(ascending=True).head(n))\n",
        "\n",
        "show_top_loadings(loadings, \"PC1\", n=5)\n",
        "show_top_loadings(loadings, \"PC2\", n=5)\n",
        "\n",
        "# Tip for report interpretation:\n",
        "# - PC dominated by ALT/AST/GGT/BIL tends to reflect \"injury/cholestasis/inflammation\"\n",
        "# - PC dominated by ALB/CHE/PROT/CHOL tends to reflect \"synthetic/metabolic function\"\n",
        "# (Direction sign is arbitrary: + vs - does not change interpretation.)\n"
      ],
      "metadata": {
        "id": "IvAkhAUhWXCF"
      },
      "id": "IvAkhAUhWXCF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.4 OVERLAY POST HOC DIAGNOSTIC CATEGORIES\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sc = plt.scatter(\n",
        "    analysis_df[\"PC1\"],\n",
        "    analysis_df[\"PC2\"],\n",
        "    c=analysis_df[\"Category_num\"],   # numeric codes for safe coloring\n",
        "    s=10,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA Projection Colored by Diagnostic Category (Post Hoc)\")\n",
        "plt.colorbar(sc, label=\"Category (numeric code)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# OPTIONAL: If you prefer readable legend with labels (can be crowded):\n",
        "def scatter_with_label_legend(df2d, title):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    for lab in sorted(df2d[\"Category_label\"].unique()):\n",
        "        mask = df2d[\"Category_label\"] == lab\n",
        "        plt.scatter(df2d.loc[mask, \"PC1\"], df2d.loc[mask, \"PC2\"], s=10, alpha=0.7, label=lab)\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.title(title)\n",
        "    plt.legend(markerscale=2, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment if you want:\n",
        "# scatter_with_label_legend(analysis_df, \"PCA Projection with Category Labels (Post Hoc)\")\n"
      ],
      "metadata": {
        "id": "bAt5dPEjWYp2"
      },
      "id": "bAt5dPEjWYp2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.5 CATEGORY DISTRIBUTION PER CLUSTER\n",
        "# ============================================================\n",
        "\n",
        "# Use numeric categories for stable ordering in tables\n",
        "cluster_category_table = pd.crosstab(\n",
        "    analysis_df[\"Cluster_KMeans\"],\n",
        "    analysis_df[\"Category_num\"],\n",
        "    normalize=\"index\"\n",
        ")\n",
        "\n",
        "print(\"\\nCategory distribution within each K-Means cluster (row-normalized):\")\n",
        "print(cluster_category_table)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(cluster_category_table.values, aspect=\"auto\")\n",
        "plt.xticks(range(len(cluster_category_table.columns)), cluster_category_table.columns)\n",
        "plt.yticks(range(len(cluster_category_table.index)), cluster_category_table.index)\n",
        "plt.xlabel(\"Category (numeric)\")\n",
        "plt.ylabel(\"Cluster (K-Means)\")\n",
        "plt.title(\"Cluster vs Category Distribution (Proportions)\")\n",
        "plt.colorbar(label=\"Proportion\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DFB5eY58WaGo"
      },
      "id": "DFB5eY58WaGo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.6 MONOTONIC TREND ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "# Compute mean PC1 per diagnostic category (numeric)\n",
        "category_means = analysis_df.groupby(\"Category_num\")[\"PC1\"].mean().sort_index()\n",
        "\n",
        "print(\"\\nMean PC1 per Category (numeric):\")\n",
        "print(category_means)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(category_means.index, category_means.values, marker=\"o\")\n",
        "plt.xlabel(\"Diagnostic Category (numeric)\")\n",
        "plt.ylabel(\"Mean PC1\")\n",
        "plt.title(\"Category Trend Along PC1 (Monotonicity Check)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# OPTIONAL: also check PC2 trend\n",
        "category_means_pc2 = analysis_df.groupby(\"Category_num\")[\"PC2\"].mean().sort_index()\n",
        "plt.figure()\n",
        "plt.plot(category_means_pc2.index, category_means_pc2.values, marker=\"o\")\n",
        "plt.xlabel(\"Diagnostic Category (numeric)\")\n",
        "plt.ylabel(\"Mean PC2\")\n",
        "plt.title(\"Category Trend Along PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OfoNPLJKWbZK"
      },
      "id": "OfoNPLJKWbZK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.7 OVERLAP ASSESSMENT (RANGES + BOX-PLOT)\n",
        "# ============================================================\n",
        "\n",
        "# Print PC1 min/max range by category to quantify overlap\n",
        "for cat in sorted(analysis_df[\"Category_num\"].unique()):\n",
        "    subset = analysis_df[analysis_df[\"Category_num\"] == cat]\n",
        "    print(f\"\\nCategory {cat} â€” PC1 range:\")\n",
        "    print(f\"Min: {subset['PC1'].min():.2f}, Max: {subset['PC1'].max():.2f}, \"\n",
        "          f\"IQR: {(subset['PC1'].quantile(0.75) - subset['PC1'].quantile(0.25)):.2f}\")\n",
        "\n",
        "# Boxplot gives an immediate visual of overlap between adjacent categories\n",
        "cats_sorted = sorted(analysis_df[\"Category_num\"].unique())\n",
        "data_for_box = [analysis_df.loc[analysis_df[\"Category_num\"] == c, \"PC1\"].values for c in cats_sorted]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.boxplot(data_for_box, labels=[str(c) for c in cats_sorted], showfliers=False)\n",
        "plt.xlabel(\"Diagnostic Category (numeric)\")\n",
        "plt.ylabel(\"PC1\")\n",
        "plt.title(\"PC1 Distribution by Category (Overlap Assessment)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# OPTIONAL: \"SEVERITY SCORE\" IN LATENT SPACE (USEFUL FOR CONCLUSION)\n",
        "# ============================================================\n",
        "# A simple unsupervised severity proxy:\n",
        "# - Use PC1 as a severity score if it shows monotonic trend with Category.\n",
        "analysis_df[\"Severity_proxy_PC1\"] = analysis_df[\"PC1\"]\n",
        "\n",
        "# Examine how clusters align with severity proxy\n",
        "severity_by_cluster = analysis_df.groupby(\"Cluster_KMeans\")[\"Severity_proxy_PC1\"].mean().sort_values()\n",
        "print(\"\\nMean severity proxy (PC1) by K-Means cluster (sorted):\")\n",
        "print(severity_by_cluster)\n"
      ],
      "metadata": {
        "id": "eO5cT7q_WdLf"
      },
      "id": "eO5cT7q_WdLf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Anomaly Detection and Atypical Profiles**"
      ],
      "metadata": {
        "id": "I_Jj7WskWe8c"
      },
      "id": "I_Jj7WskWe8c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Statistical Outlier Detection\n",
        "\n",
        "**Methods**\n",
        "\n",
        "* Z-score\n",
        "* IQR method\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Identify global deviations from population norms\n",
        "\n",
        "### 7.2 Model-based Anomaly Detection\n",
        "\n",
        "**Methods**\n",
        "\n",
        "* Isolation Forest\n",
        "* Local Outlier Factor (LOF)\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Detect local and structural anomalies\n",
        "* Identify patients poorly represented by clusters\n",
        "\n",
        "\n",
        "### Outputs\n",
        "\n",
        "* Anomaly scores per patient\n",
        "* Overlap between anomalies and advanced disease categories\n",
        "* Identification of potential data quality issues\n"
      ],
      "metadata": {
        "id": "ZFBLjno4WgD4"
      },
      "id": "ZFBLjno4WgD4"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7) ANOMALY DETECTION & ATYPICAL PROFILES (FIXED)\n",
        "# ============================================================\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ASSUMPTION\n",
        "# ------------------------------------------------------------\n",
        "# Available:\n",
        "# - X_scaled  â†’ standardized feature matrix (DataFrame)\n",
        "# - analysis_df â†’ contains PCA columns (PC1, PC2, ...) + cluster labels\n",
        "# - y_category â†’ original Category series (strings like \"0=Blood Donor\" OR numeric)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Ensure X_scaled is a DataFrame with correct index\n",
        "if isinstance(X_scaled, np.ndarray):\n",
        "    X_scaled = pd.DataFrame(X_scaled, index=analysis_df.index)\n",
        "\n",
        "# ============================================================\n",
        "# 0) ENSURE CATEGORY COLUMNS EXIST IN analysis_df (NO KEYERROR)\n",
        "# ============================================================\n",
        "\n",
        "# If analysis_df already has Category_label/Category_num from Section 6, keep them.\n",
        "# Otherwise rebuild them from y_category.\n",
        "if (\"Category_label\" not in analysis_df.columns) or (\"Category_num\" not in analysis_df.columns):\n",
        "    y_category_label = pd.Series(y_category, index=analysis_df.index).astype(str)\n",
        "\n",
        "    # Extract leading digits from labels like \"0=Blood Donor\"\n",
        "    y_category_num = pd.to_numeric(\n",
        "        y_category_label.str.extract(r\"^(\\d+)\")[0],\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "\n",
        "    # If extraction fails for some entries, map labels to codes\n",
        "    if y_category_num.isna().any():\n",
        "        label_map = {lab: i for i, lab in enumerate(sorted(y_category_label.unique()))}\n",
        "        y_category_num = y_category_label.map(label_map).astype(int)\n",
        "    else:\n",
        "        y_category_num = y_category_num.astype(int)\n",
        "\n",
        "    analysis_df[\"Category_label\"] = y_category_label.values\n",
        "    analysis_df[\"Category_num\"] = y_category_num.values\n",
        "\n"
      ],
      "metadata": {
        "id": "wnY-nwGPWktF"
      },
      "id": "wnY-nwGPWktF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.1 STATISTICAL OUTLIER DETECTION\n",
        "# ============================================================\n",
        "\n",
        "# --- Z-score method (global outliers) ---\n",
        "z_scores = np.abs(zscore(X_scaled, nan_policy=\"omit\"))\n",
        "z_outliers = (z_scores > 3).any(axis=1)\n",
        "\n",
        "analysis_df[\"Z_Outlier\"] = z_outliers.astype(int)\n",
        "print(\"\\nNumber of Z-score outliers:\", int(z_outliers.sum()))\n",
        "\n",
        "# --- IQR method (robust global outliers) ---\n",
        "Q1 = X_scaled.quantile(0.25)\n",
        "Q3 = X_scaled.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "iqr_outliers = ((X_scaled < (Q1 - 1.5 * IQR)) |\n",
        "                (X_scaled > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "\n",
        "analysis_df[\"IQR_Outlier\"] = iqr_outliers.astype(int)\n",
        "print(\"Number of IQR outliers:\", int(iqr_outliers.sum()))\n",
        "\n"
      ],
      "metadata": {
        "id": "zZUXrlrFWnGy"
      },
      "id": "zZUXrlrFWnGy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.2 MODEL-BASED ANOMALY DETECTION\n",
        "# ============================================================\n",
        "\n",
        "# --- Isolation Forest (structural/global) ---\n",
        "iso = IsolationForest(contamination=0.05, random_state=42)\n",
        "iso.fit(X_scaled)\n",
        "\n",
        "analysis_df[\"Isolation_Score\"] = iso.decision_function(X_scaled)   # higher = normal\n",
        "analysis_df[\"Isolation_Outlier\"] = (iso.predict(X_scaled) == -1).astype(int)\n",
        "\n",
        "print(\"\\nIsolation Forest detected anomalies:\", int(analysis_df[\"Isolation_Outlier\"].sum()))\n",
        "\n",
        "# --- LOF (local density anomalies) ---\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
        "lof_pred = lof.fit_predict(X_scaled)\n",
        "\n",
        "analysis_df[\"LOF_Score\"] = lof.negative_outlier_factor_\n",
        "analysis_df[\"LOF_Outlier\"] = (lof_pred == -1).astype(int)\n",
        "\n",
        "print(\"LOF detected anomalies:\", int(analysis_df[\"LOF_Outlier\"].sum()))\n"
      ],
      "metadata": {
        "id": "eRlFSiqrWokC"
      },
      "id": "eRlFSiqrWokC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.3 VISUALIZE ANOMALIES IN PCA SPACE\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sc = plt.scatter(analysis_df[\"PC1\"], analysis_df[\"PC2\"],\n",
        "                 c=analysis_df[\"Isolation_Outlier\"], s=10, alpha=0.7)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"Isolation Forest Outliers in PCA Space\")\n",
        "plt.colorbar(sc, label=\"Outlier (1=Yes)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mXjlB4Y9Wpwt"
      },
      "id": "mXjlB4Y9Wpwt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.4 OVERLAP WITH DIAGNOSTIC CATEGORIES (POST HOC)\n",
        "# ============================================================\n",
        "\n",
        "iso_category_table = pd.crosstab(\n",
        "    analysis_df[\"Isolation_Outlier\"],\n",
        "    analysis_df[\"Category_num\"],\n",
        "    normalize=\"columns\"\n",
        ")\n",
        "print(\"\\nIsolation Forest Outliers by Category (proportion within category):\")\n",
        "print(iso_category_table)\n",
        "\n",
        "lof_category_table = pd.crosstab(\n",
        "    analysis_df[\"LOF_Outlier\"],\n",
        "    analysis_df[\"Category_num\"],\n",
        "    normalize=\"columns\"\n",
        ")\n",
        "print(\"\\nLOF Outliers by Category (proportion within category):\")\n",
        "print(lof_category_table)\n"
      ],
      "metadata": {
        "id": "Vg3ZDa-AWrJ8"
      },
      "id": "Vg3ZDa-AWrJ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6) DISEASE PROGRESSION & LATENT CONTINUUM ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ASSUMPTION\n",
        "# ------------------------------------------------------------\n",
        "# You already have:\n",
        "# - X_pca_df  â†’ PCA representation (DataFrame)\n",
        "# - X_cluster â†’ dataframe containing cluster labels (DataFrame)\n",
        "# - y_category â†’ original diagnostic categories (post hoc only; may be strings like \"0=Blood Donor\")\n",
        "# - loadings â†’ PCA loadings (DataFrame)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# ============================================================\n",
        "# 0) FIX CATEGORY TYPE FOR PLOTTING (IMPORTANT)\n",
        "# ============================================================\n",
        "# Matplotlib scatter(c=...) needs numeric values, but Category might be strings.\n",
        "# We'll create both numeric and label versions:\n",
        "y_category_label = pd.Series(y_category).astype(str)\n",
        "\n",
        "# Try to extract a leading integer from labels like \"0=Blood Donor\"\n",
        "y_category_num = y_category_label.str.extract(r\"^(\\d+)\")[0]\n",
        "y_category_num = pd.to_numeric(y_category_num, errors=\"coerce\")\n",
        "\n",
        "# If extraction fails (still NaNs), map unique labels to integer codes\n",
        "if y_category_num.isna().any():\n",
        "    label_map = {lab: i for i, lab in enumerate(sorted(y_category_label.unique()))}\n",
        "    y_category_num = y_category_label.map(label_map).astype(int)\n",
        "else:\n",
        "    y_category_num = y_category_num.astype(int)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# For clarity, merge everything into one analysis dataframe\n",
        "# ------------------------------------------------------------\n",
        "analysis_df = X_pca_df.copy()\n",
        "\n",
        "analysis_df[\"Cluster_KMeans\"] = X_cluster[\"Cluster_KMeans\"].values\n",
        "analysis_df[\"Cluster_GMM\"] = X_cluster[\"Cluster_GMM\"].values\n",
        "analysis_df[\"Cluster_Hierarchical\"] = X_cluster[\"Cluster_Hierarchical\"].values\n",
        "\n",
        "analysis_df[\"Category_label\"] = y_category_label.values\n",
        "analysis_df[\"Category_num\"] = y_category_num.values\n"
      ],
      "metadata": {
        "id": "pSn1pgpwWtdy"
      },
      "id": "pSn1pgpwWtdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives\n",
        "\n",
        "* Determine whether disease structure is discrete or continuous\n",
        "* Understand cluster ordering\n",
        "\n",
        "### Methods\n",
        "\n",
        "* Project clusters onto PCA components\n",
        "* Examine cluster centroids along principal axes\n",
        "* Overlay post hoc diagnostic categories\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "* Identify monotonic trends along severity axes\n",
        "* Assess overlap between adjacent groups"
      ],
      "metadata": {
        "id": "4ejivHftWu_L"
      },
      "id": "4ejivHftWu_L"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# 6.1 PROJECT CLUSTERS ONTO PCA SPACE\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(\n",
        "    analysis_df[\"PC1\"],\n",
        "    analysis_df[\"PC2\"],\n",
        "    c=analysis_df[\"Cluster_KMeans\"],\n",
        "    s=10,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"K-Means Clusters in PCA Space\")\n",
        "plt.colorbar(label=\"Cluster (K-Means)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Compare GMM clusters in same space\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(\n",
        "    analysis_df[\"PC1\"],\n",
        "    analysis_df[\"PC2\"],\n",
        "    c=analysis_df[\"Cluster_GMM\"],\n",
        "    s=10,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"GMM Clusters in PCA Space\")\n",
        "plt.colorbar(label=\"Cluster (GMM)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D1TlsGqxW00E"
      },
      "id": "D1TlsGqxW00E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.2 EXAMINE CLUSTER CENTROIDS ALONG PRINCIPAL AXES\n",
        "# ============================================================\n",
        "\n",
        "# Compute centroids in PCA space (you can extend to PC3, PC4 if needed)\n",
        "centroids_pca = analysis_df.groupby(\"Cluster_KMeans\")[[\"PC1\", \"PC2\"]].mean()\n",
        "\n",
        "print(\"\\nCluster centroids (K-Means, PCA space):\")\n",
        "print(centroids_pca)\n",
        "\n",
        "# Plot centroids for PC1 (often a candidate severity axis)\n",
        "plt.figure()\n",
        "plt.bar(centroids_pca.index.astype(str), centroids_pca[\"PC1\"])\n",
        "plt.xlabel(\"Cluster (K-Means)\")\n",
        "plt.ylabel(\"Mean PC1\")\n",
        "plt.title(\"Cluster Ordering Along PC1 (K-Means)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# OPTIONAL: Show ordering explicitly\n",
        "centroid_order = centroids_pca[\"PC1\"].sort_values().index.tolist()\n",
        "print(\"\\nClusters ordered by increasing mean PC1 (K-Means):\", centroid_order)"
      ],
      "metadata": {
        "id": "CpSlTUdRW7lM"
      },
      "id": "CpSlTUdRW7lM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.3 LINK PCA AXES TO PHYSIOLOGY\n",
        "# ============================================================\n",
        "\n",
        "# Loadings interpretation helper: show strongest contributors for a PC\n",
        "def show_top_loadings(loadings_df, pc=\"PC1\", n=7):\n",
        "    print(f\"\\nTop + loadings for {pc}:\")\n",
        "    print(loadings_df[pc].sort_values(ascending=False).head(n))\n",
        "    print(f\"\\nTop - loadings for {pc}:\")\n",
        "    print(loadings_df[pc].sort_values(ascending=True).head(n))\n",
        "\n",
        "show_top_loadings(loadings, \"PC1\", n=5)\n",
        "show_top_loadings(loadings, \"PC2\", n=5)\n",
        "\n",
        "# Tip for report interpretation:\n",
        "# - PC dominated by ALT/AST/GGT/BIL tends to reflect \"injury/cholestasis/inflammation\"\n",
        "# - PC dominated by ALB/CHE/PROT/CHOL tends to reflect \"synthetic/metabolic function\"\n",
        "# (Direction sign is arbitrary: + vs - does not change interpretation.)\n"
      ],
      "metadata": {
        "id": "Jku5duDNW9RI"
      },
      "id": "Jku5duDNW9RI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.4 OVERLAY POST HOC DIAGNOSTIC CATEGORIES\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sc = plt.scatter(\n",
        "    analysis_df[\"PC1\"],\n",
        "    analysis_df[\"PC2\"],\n",
        "    c=analysis_df[\"Category_num\"],   # numeric codes for safe coloring\n",
        "    s=10,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA Projection Colored by Diagnostic Category (Post Hoc)\")\n",
        "plt.colorbar(sc, label=\"Category (numeric code)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# OPTIONAL: If you prefer readable legend with labels (can be crowded):\n",
        "def scatter_with_label_legend(df2d, title):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    for lab in sorted(df2d[\"Category_label\"].unique()):\n",
        "        mask = df2d[\"Category_label\"] == lab\n",
        "        plt.scatter(df2d.loc[mask, \"PC1\"], df2d.loc[mask, \"PC2\"], s=10, alpha=0.7, label=lab)\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.title(title)\n",
        "    plt.legend(markerscale=2, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment if you want:\n",
        "# scatter_with_label_legend(analysis_df, \"PCA Projection with Category Labels (Post Hoc)\")\n"
      ],
      "metadata": {
        "id": "9uLp-jR4W-6y"
      },
      "id": "9uLp-jR4W-6y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.5 CATEGORY DISTRIBUTION PER CLUSTER\n",
        "# ============================================================\n",
        "\n",
        "# Use numeric categories for stable ordering in tables\n",
        "cluster_category_table = pd.crosstab(\n",
        "    analysis_df[\"Cluster_KMeans\"],\n",
        "    analysis_df[\"Category_num\"],\n",
        "    normalize=\"index\"\n",
        ")\n",
        "\n",
        "print(\"\\nCategory distribution within each K-Means cluster (row-normalized):\")\n",
        "print(cluster_category_table)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(cluster_category_table.values, aspect=\"auto\")\n",
        "plt.xticks(range(len(cluster_category_table.columns)), cluster_category_table.columns)\n",
        "plt.yticks(range(len(cluster_category_table.index)), cluster_category_table.index)\n",
        "plt.xlabel(\"Category (numeric)\")\n",
        "plt.ylabel(\"Cluster (K-Means)\")\n",
        "plt.title(\"Cluster vs Category Distribution (Proportions)\")\n",
        "plt.colorbar(label=\"Proportion\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NYiqtwLkXAhe"
      },
      "id": "NYiqtwLkXAhe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.6 MONOTONIC TREND ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "# Compute mean PC1 per diagnostic category (numeric)\n",
        "category_means = analysis_df.groupby(\"Category_num\")[\"PC1\"].mean().sort_index()\n",
        "\n",
        "print(\"\\nMean PC1 per Category (numeric):\")\n",
        "print(category_means)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(category_means.index, category_means.values, marker=\"o\")\n",
        "plt.xlabel(\"Diagnostic Category (numeric)\")\n",
        "plt.ylabel(\"Mean PC1\")\n",
        "plt.title(\"Category Trend Along PC1 (Monotonicity Check)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# OPTIONAL: also check PC2 trend\n",
        "category_means_pc2 = analysis_df.groupby(\"Category_num\")[\"PC2\"].mean().sort_index()\n",
        "plt.figure()\n",
        "plt.plot(category_means_pc2.index, category_means_pc2.values, marker=\"o\")\n",
        "plt.xlabel(\"Diagnostic Category (numeric)\")\n",
        "plt.ylabel(\"Mean PC2\")\n",
        "plt.title(\"Category Trend Along PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sawq4aJ_XCLZ"
      },
      "id": "Sawq4aJ_XCLZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.7 OVERLAP ASSESSMENT (RANGES + BOX-PLOT)\n",
        "# ============================================================\n",
        "\n",
        "# Print PC1 min/max range by category to quantify overlap\n",
        "for cat in sorted(analysis_df[\"Category_num\"].unique()):\n",
        "    subset = analysis_df[analysis_df[\"Category_num\"] == cat]\n",
        "    print(f\"\\nCategory {cat} â€” PC1 range:\")\n",
        "    print(f\"Min: {subset['PC1'].min():.2f}, Max: {subset['PC1'].max():.2f}, \"\n",
        "          f\"IQR: {(subset['PC1'].quantile(0.75) - subset['PC1'].quantile(0.25)):.2f}\")\n",
        "\n",
        "# Boxplot gives an immediate visual of overlap between adjacent categories\n",
        "cats_sorted = sorted(analysis_df[\"Category_num\"].unique())\n",
        "data_for_box = [analysis_df.loc[analysis_df[\"Category_num\"] == c, \"PC1\"].values for c in cats_sorted]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.boxplot(data_for_box, labels=[str(c) for c in cats_sorted], showfliers=False)\n",
        "plt.xlabel(\"Diagnostic Category (numeric)\")\n",
        "plt.ylabel(\"PC1\")\n",
        "plt.title(\"PC1 Distribution by Category (Overlap Assessment)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# OPTIONAL: \"SEVERITY SCORE\" IN LATENT SPACE (USEFUL FOR CONCLUSION)\n",
        "# ============================================================\n",
        "# A simple unsupervised severity proxy:\n",
        "# - Use PC1 as a severity score if it shows monotonic trend with Category.\n",
        "analysis_df[\"Severity_proxy_PC1\"] = analysis_df[\"PC1\"]\n",
        "\n",
        "# Examine how clusters align with severity proxy\n",
        "severity_by_cluster = analysis_df.groupby(\"Cluster_KMeans\")[\"Severity_proxy_PC1\"].mean().sort_values()\n",
        "print(\"\\nMean severity proxy (PC1) by K-Means cluster (sorted):\")\n",
        "print(severity_by_cluster)\n"
      ],
      "metadata": {
        "id": "lRA2VepSXDrf"
      },
      "id": "lRA2VepSXDrf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Anomaly Detection and Atypical Profiles**"
      ],
      "metadata": {
        "id": "7m8iD2g4XFye"
      },
      "id": "7m8iD2g4XFye"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Statistical Outlier Detection\n",
        "\n",
        "**Methods**\n",
        "\n",
        "* Z-score\n",
        "* IQR method\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Identify global deviations from population norms\n",
        "\n",
        "### 7.2 Model-based Anomaly Detection\n",
        "\n",
        "**Methods**\n",
        "\n",
        "* Isolation Forest\n",
        "* Local Outlier Factor (LOF)\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "* Detect local and structural anomalies\n",
        "* Identify patients poorly represented by clusters\n",
        "\n",
        "\n",
        "### Outputs\n",
        "\n",
        "* Anomaly scores per patient\n",
        "* Overlap between anomalies and advanced disease categories\n",
        "* Identification of potential data quality issues\n"
      ],
      "metadata": {
        "id": "8yU8H6WuXHW_"
      },
      "id": "8yU8H6WuXHW_"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7) ANOMALY DETECTION & ATYPICAL PROFILES (FIXED)\n",
        "# ============================================================\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ASSUMPTION\n",
        "# ------------------------------------------------------------\n",
        "# Available:\n",
        "# - X_scaled  â†’ standardized feature matrix (DataFrame)\n",
        "# - analysis_df â†’ contains PCA columns (PC1, PC2, ...) + cluster labels\n",
        "# - y_category â†’ original Category series (strings like \"0=Blood Donor\" OR numeric)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Ensure X_scaled is a DataFrame with correct index\n",
        "if isinstance(X_scaled, np.ndarray):\n",
        "    X_scaled = pd.DataFrame(X_scaled, index=analysis_df.index)\n",
        "\n",
        "# ============================================================\n",
        "# 0) ENSURE CATEGORY COLUMNS EXIST IN analysis_df (NO KEYERROR)\n",
        "# ============================================================\n",
        "\n",
        "# If analysis_df already has Category_label/Category_num from Section 6, keep them.\n",
        "# Otherwise rebuild them from y_category.\n",
        "if (\"Category_label\" not in analysis_df.columns) or (\"Category_num\" not in analysis_df.columns):\n",
        "    y_category_label = pd.Series(y_category, index=analysis_df.index).astype(str)\n",
        "\n",
        "    # Extract leading digits from labels like \"0=Blood Donor\"\n",
        "    y_category_num = pd.to_numeric(\n",
        "        y_category_label.str.extract(r\"^(\\d+)\")[0],\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "\n",
        "    # If extraction fails for some entries, map labels to codes\n",
        "    if y_category_num.isna().any():\n",
        "        label_map = {lab: i for i, lab in enumerate(sorted(y_category_label.unique()))}\n",
        "        y_category_num = y_category_label.map(label_map).astype(int)\n",
        "    else:\n",
        "        y_category_num = y_category_num.astype(int)\n",
        "\n",
        "    analysis_df[\"Category_label\"] = y_category_label.values\n",
        "    analysis_df[\"Category_num\"] = y_category_num.values\n",
        "\n"
      ],
      "metadata": {
        "id": "-0zjXw1iXIor"
      },
      "id": "-0zjXw1iXIor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.1 STATISTICAL OUTLIER DETECTION\n",
        "# ============================================================\n",
        "\n",
        "# --- Z-score method (global outliers) ---\n",
        "z_scores = np.abs(zscore(X_scaled, nan_policy=\"omit\"))\n",
        "z_outliers = (z_scores > 3).any(axis=1)\n",
        "\n",
        "analysis_df[\"Z_Outlier\"] = z_outliers.astype(int)\n",
        "print(\"\\nNumber of Z-score outliers:\", int(z_outliers.sum()))\n",
        "\n",
        "# --- IQR method (robust global outliers) ---\n",
        "Q1 = X_scaled.quantile(0.25)\n",
        "Q3 = X_scaled.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "iqr_outliers = ((X_scaled < (Q1 - 1.5 * IQR)) |\n",
        "                (X_scaled > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "\n",
        "analysis_df[\"IQR_Outlier\"] = iqr_outliers.astype(int)\n",
        "print(\"Number of IQR outliers:\", int(iqr_outliers.sum()))\n",
        "\n"
      ],
      "metadata": {
        "id": "xIsH-E6qXKS-"
      },
      "id": "xIsH-E6qXKS-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.2 MODEL-BASED ANOMALY DETECTION\n",
        "# ============================================================\n",
        "\n",
        "# --- Isolation Forest (structural/global) ---\n",
        "iso = IsolationForest(contamination=0.05, random_state=42)\n",
        "iso.fit(X_scaled)\n",
        "\n",
        "analysis_df[\"Isolation_Score\"] = iso.decision_function(X_scaled)   # higher = normal\n",
        "analysis_df[\"Isolation_Outlier\"] = (iso.predict(X_scaled) == -1).astype(int)\n",
        "\n",
        "print(\"\\nIsolation Forest detected anomalies:\", int(analysis_df[\"Isolation_Outlier\"].sum()))\n",
        "\n",
        "# --- LOF (local density anomalies) ---\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
        "lof_pred = lof.fit_predict(X_scaled)\n",
        "\n",
        "analysis_df[\"LOF_Score\"] = lof.negative_outlier_factor_\n",
        "analysis_df[\"LOF_Outlier\"] = (lof_pred == -1).astype(int)\n",
        "\n",
        "print(\"LOF detected anomalies:\", int(analysis_df[\"LOF_Outlier\"].sum()))\n"
      ],
      "metadata": {
        "id": "6BrybKGjXLe9"
      },
      "id": "6BrybKGjXLe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.3 VISUALIZE ANOMALIES IN PCA SPACE\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sc = plt.scatter(analysis_df[\"PC1\"], analysis_df[\"PC2\"],\n",
        "                 c=analysis_df[\"Isolation_Outlier\"], s=10, alpha=0.7)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"Isolation Forest Outliers in PCA Space\")\n",
        "plt.colorbar(sc, label=\"Outlier (1=Yes)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hrdz1f0vXMrd"
      },
      "id": "Hrdz1f0vXMrd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.4 OVERLAP WITH DIAGNOSTIC CATEGORIES (POST HOC)\n",
        "# ============================================================\n",
        "\n",
        "iso_category_table = pd.crosstab(\n",
        "    analysis_df[\"Isolation_Outlier\"],\n",
        "    analysis_df[\"Category_num\"],\n",
        "    normalize=\"columns\"\n",
        ")\n",
        "print(\"\\nIsolation Forest Outliers by Category (proportion within category):\")\n",
        "print(iso_category_table)\n",
        "\n",
        "lof_category_table = pd.crosstab(\n",
        "    analysis_df[\"LOF_Outlier\"],\n",
        "    analysis_df[\"Category_num\"],\n",
        "    normalize=\"columns\"\n",
        ")\n",
        "print(\"\\nLOF Outliers by Category (proportion within category):\")\n",
        "print(lof_category_table)\n"
      ],
      "metadata": {
        "id": "gP5qxN3aXOJu"
      },
      "id": "gP5qxN3aXOJu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.5 COMBINED ANOMALY FLAG\n",
        "# ============================================================\n",
        "\n",
        "analysis_df[\"Any_Anomaly\"] = (\n",
        "    analysis_df[\"Z_Outlier\"] |\n",
        "    analysis_df[\"IQR_Outlier\"] |\n",
        "    analysis_df[\"Isolation_Outlier\"] |\n",
        "    analysis_df[\"LOF_Outlier\"]\n",
        ").astype(int)\n",
        "\n",
        "print(\"\\nTotal patients flagged by any method:\", int(analysis_df[\"Any_Anomaly\"].sum()))\n"
      ],
      "metadata": {
        "id": "ebkJQGjLXPvP"
      },
      "id": "ebkJQGjLXPvP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.6 IDENTIFY EXTREME PROFILES\n",
        "# ============================================================\n",
        "\n",
        "most_anomalous = analysis_df.sort_values(\"Isolation_Score\").head(10)\n",
        "\n",
        "print(\"\\nTop 10 most anomalous patients (Isolation Forest):\")\n",
        "print(most_anomalous[[\"Isolation_Score\", \"LOF_Score\", \"Category_label\", \"Category_num\"]])\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(analysis_df[\"PC1\"], analysis_df[\"PC2\"], s=10, alpha=0.25)\n",
        "plt.scatter(most_anomalous[\"PC1\"], most_anomalous[\"PC2\"], s=45, alpha=0.9)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"Top 10 Anomalies Highlighted in PCA Space\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "duDmPOWJXQ9X"
      },
      "id": "duDmPOWJXQ9X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Post Hoc Clinical Interpretation**"
      ],
      "metadata": {
        "id": "ZFw1oPTrXTVq"
      },
      "id": "ZFw1oPTrXTVq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use of Category variable\n",
        "\n",
        "* Compare cluster membership with diagnostic categories\n",
        "* Analyze distribution of categories per cluster\n",
        "* Assess agreement without claiming causality\n",
        "\n",
        "ðŸ“Œ *Important:* This step is interpretative, not evaluative."
      ],
      "metadata": {
        "id": "KOjLckfXXSuD"
      },
      "id": "KOjLckfXXSuD"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8.1 Cluster vs Category Distribution (K-Means)\n",
        "# ============================================================\n",
        "\n",
        "# Cross-tabulation (counts)\n",
        "cluster_vs_category_counts = pd.crosstab(\n",
        "    analysis_df[\"Cluster_KMeans\"],\n",
        "    analysis_df[\"Category_label\"]\n",
        ")\n",
        "\n",
        "print(\"\\nCluster vs Category (Counts):\")\n",
        "print(cluster_vs_category_counts)\n",
        "\n",
        "# Row-normalized proportions (within cluster)\n",
        "cluster_vs_category_prop = pd.crosstab(\n",
        "    analysis_df[\"Cluster_KMeans\"],\n",
        "    analysis_df[\"Category_label\"],\n",
        "    normalize=\"index\"\n",
        ")\n",
        "\n",
        "print(\"\\nCluster vs Category (Proportions within cluster):\")\n",
        "print(cluster_vs_category_prop)\n"
      ],
      "metadata": {
        "id": "zWJp-1XFXVQj"
      },
      "id": "zWJp-1XFXVQj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8.2 Dominant Category per Cluster\n",
        "# ============================================================\n",
        "\n",
        "dominant_category = cluster_vs_category_prop.idxmax(axis=1)\n",
        "\n",
        "print(\"\\nDominant diagnostic category per cluster:\")\n",
        "print(dominant_category)\n"
      ],
      "metadata": {
        "id": "PYmpf5oSXXRj"
      },
      "id": "PYmpf5oSXXRj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8.3 Cluster Purity (Descriptive, NOT predictive accuracy)\n",
        "# ============================================================\n",
        "\n",
        "# Purity = max proportion in each cluster\n",
        "cluster_purity = cluster_vs_category_prop.max(axis=1)\n",
        "\n",
        "print(\"\\nCluster purity (max proportion per cluster):\")\n",
        "print(cluster_purity)\n",
        "\n",
        "print(\"\\nAverage cluster purity:\",\n",
        "      cluster_purity.mean())"
      ],
      "metadata": {
        "id": "bDMt0YQXXYln"
      },
      "id": "bDMt0YQXXYln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8.4 Adjusted Rand Index (Optional, descriptive only)\n",
        "# ============================================================\n",
        "\n",
        "# This measures similarity between cluster assignment and diagnostic labels.\n",
        "# IMPORTANT: This is descriptive similarity, NOT predictive accuracy.\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "ari = adjusted_rand_score(\n",
        "    analysis_df[\"Category_num\"],\n",
        "    analysis_df[\"Cluster_KMeans\"]\n",
        ")\n",
        "\n",
        "print(\"\\nAdjusted Rand Index (Cluster vs Category):\", ari)\n"
      ],
      "metadata": {
        "id": "ONflt3ZdXZ6f"
      },
      "id": "ONflt3ZdXZ6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Robustness and Sensitivity Analysis**"
      ],
      "metadata": {
        "id": "FaODquyfXbd5"
      },
      "id": "FaODquyfXbd5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives\n",
        "\n",
        "* Ensure findings are not artifacts of preprocessing or algorithm choice\n",
        "\n",
        "### Methods\n",
        "\n",
        "* Compare clustering across:\n",
        "\n",
        "  * Different numbers of PCA components\n",
        "  * Different algorithms (K-means vs GMM)\n",
        "* Assess stability using silhouette score trends"
      ],
      "metadata": {
        "id": "yeTYScjvXdDQ"
      },
      "id": "yeTYScjvXdDQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9.1 Sensitivity to Number of PCA Components\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n--- Sensitivity to PCA dimensionality ---\")\n",
        "\n",
        "pca_components_test = [2, 3, 4, 5, 6]\n",
        "silhouette_results = []\n",
        "\n",
        "for n_comp in pca_components_test:\n",
        "\n",
        "    # Fit PCA with fixed number of components\n",
        "    pca_test = PCA(n_components=n_comp, random_state=42)\n",
        "    X_pca_test = pca_test.fit_transform(X_scaled)\n",
        "\n",
        "    # K-Means clustering\n",
        "    kmeans_test = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)\n",
        "    labels_test = kmeans_test.fit_predict(X_pca_test)\n",
        "\n",
        "    sil = silhouette_score(X_pca_test, labels_test)\n",
        "    silhouette_results.append(sil)\n",
        "\n",
        "    print(f\"PCA components: {n_comp}, Silhouette: {sil:.4f}\")\n",
        "\n",
        "# Plot stability\n",
        "plt.figure()\n",
        "plt.plot(pca_components_test, silhouette_results, marker=\"o\")\n",
        "plt.xlabel(\"Number of PCA Components\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Clustering Stability Across PCA Dimensions\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f_rxy7FZXcrL"
      },
      "id": "f_rxy7FZXcrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9.2 Compare Algorithms (K-Means vs GMM)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n--- Algorithm comparison (K-Means vs GMM) ---\")\n",
        "\n",
        "# Use your previously selected PCA representation\n",
        "X_for_compare = X_pca_df.copy()\n",
        "\n",
        "# K-Means\n",
        "kmeans_labels = analysis_df[\"Cluster_KMeans\"]\n",
        "\n",
        "# GMM\n",
        "gmm_labels = analysis_df[\"Cluster_GMM\"]\n",
        "\n",
        "# Agreement between clusterings\n",
        "ari_between_algorithms = adjusted_rand_score(kmeans_labels, gmm_labels)\n",
        "\n",
        "print(\"Adjusted Rand Index (KMeans vs GMM):\", ari_between_algorithms)\n",
        "\n"
      ],
      "metadata": {
        "id": "57eTgnpEXiCx"
      },
      "id": "57eTgnpEXiCx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9.3 Silhouette Comparison Across Algorithms\n",
        "# ============================================================\n",
        "\n",
        "sil_kmeans = silhouette_score(X_for_compare, kmeans_labels)\n",
        "sil_gmm = silhouette_score(X_for_compare, gmm_labels)\n",
        "\n",
        "print(\"\\nSilhouette Scores:\")\n",
        "print(\"K-Means:\", sil_kmeans)\n",
        "print(\"GMM:\", sil_gmm)\n"
      ],
      "metadata": {
        "id": "Xcg4k_tWXjTc"
      },
      "id": "Xcg4k_tWXjTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9.4 Interpretation Support Tables\n",
        "# ============================================================\n",
        "\n",
        "comparison_table = pd.DataFrame({\n",
        "    \"Method\": [\"K-Means\", \"GMM\"],\n",
        "    \"Silhouette\": [sil_kmeans, sil_gmm]\n",
        "})\n",
        "\n",
        "print(\"\\nClustering Method Comparison:\")\n",
        "print(comparison_table)\n"
      ],
      "metadata": {
        "id": "IsQISRwZXkqK"
      },
      "id": "IsQISRwZXkqK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10. Limitations and Methodological Constraints**"
      ],
      "metadata": {
        "id": "IdMAcFKUXl7I"
      },
      "id": "IdMAcFKUXl7I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Explicitly acknowledge:\n",
        "\n",
        "* Observational nature of data\n",
        "* Overlap between disease stages\n",
        "* Sensitivity to scaling and transformations\n",
        "* Absence of longitudinal information"
      ],
      "metadata": {
        "id": "OT5scbRRXnQL"
      },
      "id": "OT5scbRRXnQL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11. Conclusions â€“ How Everything Comes Together**"
      ],
      "metadata": {
        "id": "3ATvxRlgXpj2"
      },
      "id": "3ATvxRlgXpj2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Your conclusions should answer:\n",
        "\n",
        "1. **Structure:**\n",
        "   The data exhibits clear latent structure driven by liver inflammation and synthetic function markers.\n",
        "\n",
        "2. **Stratification:**\n",
        "   Unsupervised clustering reveals coherent patient subgroups, though boundaries are fuzzy and overlapping.\n",
        "\n",
        "3. **Progression:**\n",
        "   Disease severity appears as a **continuum rather than strictly discrete stages**.\n",
        "\n",
        "4. **Anomalies:**\n",
        "   Atypical profiles exist and often correspond to severe or complex cases not well captured by simple clustering.\n",
        "\n",
        "5. **Value of unsupervised learning:**\n",
        "   Unsupervised methods provide complementary insight to clinical classification by revealing heterogeneity and uncertainty.\n",
        "\n"
      ],
      "metadata": {
        "id": "mg1xi5MVXq9e"
      },
      "id": "mg1xi5MVXq9e"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0PkTPiQqXo0A"
      },
      "id": "0PkTPiQqXo0A",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}